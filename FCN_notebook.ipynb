{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FCN_notebook.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM62uUY0SxYRKubxfRv6JoB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cb9b3d2218d146819793028e46fc3492":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6685210eb6334c258841f856e35b2c2d","IPY_MODEL_3e86a4cfa79842c2aa2564b3a0ae67d0","IPY_MODEL_b94f4035e3cc48bab193d6c54a9f0fad"],"layout":"IPY_MODEL_f9b745fe681e4b7a8239e87a28a8c5ad"}},"6685210eb6334c258841f856e35b2c2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1db7a3df340472d8f3ae50e97d74c4f","placeholder":"​","style":"IPY_MODEL_84cb7ba6f65d45eda3f87382a3211854","value":"100%"}},"3e86a4cfa79842c2aa2564b3a0ae67d0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c566d9ec90e4ee192a0060b1318caa6","max":553433881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f6a0e599b7e4bcea8fe307547291de3","value":553433881}},"b94f4035e3cc48bab193d6c54a9f0fad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fc8d239f1e9439d8794a5786c2306eb","placeholder":"​","style":"IPY_MODEL_7192777dba634242894acc803b2fc95c","value":" 528M/528M [00:04&lt;00:00, 120MB/s]"}},"f9b745fe681e4b7a8239e87a28a8c5ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1db7a3df340472d8f3ae50e97d74c4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84cb7ba6f65d45eda3f87382a3211854":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c566d9ec90e4ee192a0060b1318caa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6a0e599b7e4bcea8fe307547291de3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2fc8d239f1e9439d8794a5786c2306eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7192777dba634242894acc803b2fc95c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3eGV8ku_zT9","executionInfo":{"status":"ok","timestamp":1648723948726,"user_tz":-480,"elapsed":17367,"user":{"displayName":"Junyong Li","userId":"01294882104453873440"}},"outputId":"7a0b15c5-5be5-495c-d874-e3a2cd19f94d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QC5zkaguQ-ka","executionInfo":{"status":"ok","timestamp":1648723962920,"user_tz":-480,"elapsed":369,"user":{"displayName":"Junyong Li","userId":"01294882104453873440"}},"outputId":"0edc05df-d65f-4a67-fb1b-8bf1ff87ec66"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/FCN\n"]},{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":2}],"source":["%cd drive/MyDrive/FCN\n","%dirs"]},{"cell_type":"code","source":["%run -i train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cb9b3d2218d146819793028e46fc3492","6685210eb6334c258841f856e35b2c2d","3e86a4cfa79842c2aa2564b3a0ae67d0","b94f4035e3cc48bab193d6c54a9f0fad","f9b745fe681e4b7a8239e87a28a8c5ad","e1db7a3df340472d8f3ae50e97d74c4f","84cb7ba6f65d45eda3f87382a3211854","0c566d9ec90e4ee192a0060b1318caa6","5f6a0e599b7e4bcea8fe307547291de3","2fc8d239f1e9439d8794a5786c2306eb","7192777dba634242894acc803b2fc95c"]},"id":"80KjFPJlRYWS","outputId":"5224dc54-d0d7-410b-d652-ff7c4f0e9e23","executionInfo":{"status":"ok","timestamp":1648723476298,"user_tz":-480,"elapsed":4696648,"user":{"displayName":"Junyong Li","userId":"01294882104453873440"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/528M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb9b3d2218d146819793028e46fc3492"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2022-03-31 09:26:47,025: INFO: [train.py:226]: Start training: Total epochs: 80, Batch size: 16, Training size: 648, Validation size: 72\n","2022-03-31 09:26:47,027: INFO: [train.py:41]: Epoch 001, Learning Rate 0.01\n","2022-03-31 09:30:39,645: INFO: [train.py:79]: Batch Loss: 0.336162 Time: 232 s\n","2022-03-31 09:31:00,706: INFO: [train.py:117]: Loss: 1.958754 Time: 21\n","2022-03-31 09:31:00,708: INFO: [train.py:41]: Epoch 002, Learning Rate 0.01\n","2022-03-31 09:31:54,645: INFO: [train.py:79]: Batch Loss: 0.090083 Time: 53 s\n","2022-03-31 09:31:57,140: INFO: [train.py:117]: Loss: 0.614533 Time: 2\n","2022-03-31 09:31:57,142: INFO: [train.py:41]: Epoch 003, Learning Rate 0.01\n","2022-03-31 09:32:50,722: INFO: [train.py:79]: Batch Loss: 0.075674 Time: 53 s\n","2022-03-31 09:32:53,217: INFO: [train.py:117]: Loss: 0.419660 Time: 2\n","2022-03-31 09:32:53,220: INFO: [train.py:41]: Epoch 004, Learning Rate 0.01\n","2022-03-31 09:33:46,730: INFO: [train.py:79]: Batch Loss: 0.062530 Time: 53 s\n","2022-03-31 09:33:49,248: INFO: [train.py:117]: Loss: 0.339584 Time: 2\n","2022-03-31 09:33:49,250: INFO: [train.py:41]: Epoch 005, Learning Rate 0.01\n","2022-03-31 09:34:42,713: INFO: [train.py:79]: Batch Loss: 0.056951 Time: 53 s\n","2022-03-31 09:34:45,210: INFO: [train.py:117]: Loss: 0.417431 Time: 2\n","2022-03-31 09:34:45,212: INFO: [train.py:41]: Epoch 006, Learning Rate 0.01\n","2022-03-31 09:35:39,045: INFO: [train.py:79]: Batch Loss: 0.053436 Time: 53 s\n","2022-03-31 09:35:41,549: INFO: [train.py:117]: Loss: 0.275865 Time: 2\n","2022-03-31 09:35:41,552: INFO: [train.py:41]: Epoch 007, Learning Rate 0.01\n","2022-03-31 09:36:35,135: INFO: [train.py:79]: Batch Loss: 0.049958 Time: 53 s\n","2022-03-31 09:36:37,674: INFO: [train.py:117]: Loss: 0.549373 Time: 2\n","2022-03-31 09:36:37,676: INFO: [train.py:41]: Epoch 008, Learning Rate 0.01\n","2022-03-31 09:37:31,233: INFO: [train.py:79]: Batch Loss: 0.050326 Time: 53 s\n","2022-03-31 09:37:33,690: INFO: [train.py:117]: Loss: 0.299007 Time: 2\n","2022-03-31 09:37:33,692: INFO: [train.py:41]: Epoch 009, Learning Rate 0.01\n","2022-03-31 09:38:27,078: INFO: [train.py:79]: Batch Loss: 0.048154 Time: 53 s\n","2022-03-31 09:38:29,545: INFO: [train.py:117]: Loss: 0.272130 Time: 2\n","2022-03-31 09:38:29,547: INFO: [train.py:41]: Epoch 010, Learning Rate 0.01\n","2022-03-31 09:39:23,020: INFO: [train.py:79]: Batch Loss: 0.044942 Time: 53 s\n","2022-03-31 09:39:25,520: INFO: [train.py:117]: Loss: 0.380452 Time: 2\n","2022-03-31 09:39:25,523: INFO: [train.py:41]: Epoch 011, Learning Rate 0.01\n","2022-03-31 09:40:18,952: INFO: [train.py:79]: Batch Loss: 0.042991 Time: 53 s\n","2022-03-31 09:40:21,399: INFO: [train.py:117]: Loss: 0.268137 Time: 2\n","2022-03-31 09:40:21,402: INFO: [train.py:41]: Epoch 012, Learning Rate 0.01\n","2022-03-31 09:41:14,910: INFO: [train.py:79]: Batch Loss: 0.043378 Time: 53 s\n","2022-03-31 09:41:17,402: INFO: [train.py:117]: Loss: 33656.521565 Time: 2\n","2022-03-31 09:41:17,404: INFO: [train.py:41]: Epoch 013, Learning Rate 0.01\n","2022-03-31 09:42:10,935: INFO: [train.py:79]: Batch Loss: 0.044172 Time: 53 s\n","2022-03-31 09:42:13,429: INFO: [train.py:117]: Loss: 14.354378 Time: 2\n","2022-03-31 09:42:13,431: INFO: [train.py:41]: Epoch 014, Learning Rate 0.01\n","2022-03-31 09:43:06,824: INFO: [train.py:79]: Batch Loss: 0.044962 Time: 53 s\n","2022-03-31 09:43:09,302: INFO: [train.py:117]: Loss: 0.331448 Time: 2\n","2022-03-31 09:43:09,303: INFO: [train.py:41]: Epoch 015, Learning Rate 0.01\n","2022-03-31 09:44:02,857: INFO: [train.py:79]: Batch Loss: 0.040770 Time: 53 s\n","2022-03-31 09:44:05,360: INFO: [train.py:117]: Loss: 0.336705 Time: 2\n","2022-03-31 09:44:05,367: INFO: [train.py:41]: Epoch 016, Learning Rate 0.01\n","2022-03-31 09:44:58,797: INFO: [train.py:79]: Batch Loss: 0.039026 Time: 53 s\n","2022-03-31 09:45:01,263: INFO: [train.py:117]: Loss: 3.157186 Time: 2\n","2022-03-31 09:45:01,266: INFO: [train.py:41]: Epoch 017, Learning Rate 0.01\n","2022-03-31 09:45:54,719: INFO: [train.py:79]: Batch Loss: 0.040219 Time: 53 s\n","2022-03-31 09:45:57,201: INFO: [train.py:117]: Loss: 0.559086 Time: 2\n","2022-03-31 09:45:57,204: INFO: [train.py:41]: Epoch 018, Learning Rate 0.01\n","2022-03-31 09:46:50,875: INFO: [train.py:79]: Batch Loss: 0.038775 Time: 53 s\n","2022-03-31 09:46:53,358: INFO: [train.py:117]: Loss: 780859072.225799 Time: 2\n","2022-03-31 09:46:53,365: INFO: [train.py:41]: Epoch 019, Learning Rate 0.01\n","2022-03-31 09:47:46,967: INFO: [train.py:79]: Batch Loss: 0.040282 Time: 53 s\n","2022-03-31 09:47:49,498: INFO: [train.py:117]: Loss: 0.433079 Time: 2\n","2022-03-31 09:47:49,501: INFO: [train.py:41]: Epoch 020, Learning Rate 0.01\n","2022-03-31 09:48:43,168: INFO: [train.py:79]: Batch Loss: 0.037008 Time: 53 s\n","2022-03-31 09:48:45,704: INFO: [train.py:117]: Loss: 0.313703 Time: 2\n","2022-03-31 09:48:45,706: INFO: [train.py:41]: Epoch 021, Learning Rate 0.01\n","2022-03-31 09:49:39,230: INFO: [train.py:79]: Batch Loss: 0.037069 Time: 53 s\n","2022-03-31 09:49:41,711: INFO: [train.py:117]: Loss: 0.370599 Time: 2\n","2022-03-31 09:49:41,714: INFO: [train.py:41]: Epoch 022, Learning Rate 0.01\n","2022-03-31 09:50:35,318: INFO: [train.py:79]: Batch Loss: 0.036788 Time: 53 s\n","2022-03-31 09:50:37,832: INFO: [train.py:117]: Loss: 0.306370 Time: 2\n","2022-03-31 09:50:37,834: INFO: [train.py:41]: Epoch 023, Learning Rate 0.01\n","2022-03-31 09:51:31,463: INFO: [train.py:79]: Batch Loss: 0.035557 Time: 53 s\n","2022-03-31 09:51:34,001: INFO: [train.py:117]: Loss: 0.510162 Time: 2\n","2022-03-31 09:51:34,008: INFO: [train.py:41]: Epoch 024, Learning Rate 0.01\n","2022-03-31 09:52:27,631: INFO: [train.py:79]: Batch Loss: 0.035513 Time: 53 s\n","2022-03-31 09:52:30,167: INFO: [train.py:117]: Loss: 0.307638 Time: 2\n","2022-03-31 09:52:30,170: INFO: [train.py:41]: Epoch 025, Learning Rate 0.01\n","2022-03-31 09:53:24,035: INFO: [train.py:79]: Batch Loss: 0.032559 Time: 53 s\n","2022-03-31 09:53:26,538: INFO: [train.py:117]: Loss: 0.446683 Time: 2\n","2022-03-31 09:53:26,542: INFO: [train.py:41]: Epoch 026, Learning Rate 0.01\n","2022-03-31 09:54:20,145: INFO: [train.py:79]: Batch Loss: 0.031903 Time: 53 s\n","2022-03-31 09:54:22,644: INFO: [train.py:117]: Loss: 0.450164 Time: 2\n","2022-03-31 09:54:22,646: INFO: [train.py:41]: Epoch 027, Learning Rate 0.01\n","2022-03-31 09:55:16,048: INFO: [train.py:79]: Batch Loss: 0.030459 Time: 53 s\n","2022-03-31 09:55:18,567: INFO: [train.py:117]: Loss: 0.339369 Time: 2\n","2022-03-31 09:55:18,569: INFO: [train.py:41]: Epoch 028, Learning Rate 0.01\n","2022-03-31 09:56:12,271: INFO: [train.py:79]: Batch Loss: 0.030944 Time: 53 s\n","2022-03-31 09:56:14,774: INFO: [train.py:117]: Loss: 0.404924 Time: 2\n","2022-03-31 09:56:14,776: INFO: [train.py:41]: Epoch 029, Learning Rate 0.01\n","2022-03-31 09:57:08,157: INFO: [train.py:79]: Batch Loss: 0.034727 Time: 53 s\n","2022-03-31 09:57:10,635: INFO: [train.py:117]: Loss: 0.268221 Time: 2\n","2022-03-31 09:57:10,641: INFO: [train.py:41]: Epoch 030, Learning Rate 0.01\n","2022-03-31 09:58:04,185: INFO: [train.py:79]: Batch Loss: 0.034260 Time: 53 s\n","2022-03-31 09:58:06,686: INFO: [train.py:117]: Loss: 0.395160 Time: 2\n","2022-03-31 09:58:06,688: INFO: [train.py:41]: Epoch 031, Learning Rate 0.01\n","2022-03-31 09:59:00,129: INFO: [train.py:79]: Batch Loss: 0.039822 Time: 53 s\n","2022-03-31 09:59:02,622: INFO: [train.py:117]: Loss: 0.513978 Time: 2\n","2022-03-31 09:59:02,626: INFO: [train.py:41]: Epoch 032, Learning Rate 0.01\n","2022-03-31 09:59:56,101: INFO: [train.py:79]: Batch Loss: 0.039713 Time: 53 s\n","2022-03-31 09:59:58,554: INFO: [train.py:117]: Loss: 0.445592 Time: 2\n","2022-03-31 09:59:58,555: INFO: [train.py:41]: Epoch 033, Learning Rate 0.01\n","2022-03-31 10:00:51,935: INFO: [train.py:79]: Batch Loss: 0.037452 Time: 53 s\n","2022-03-31 10:00:54,390: INFO: [train.py:117]: Loss: 0.887513 Time: 2\n","2022-03-31 10:00:54,393: INFO: [train.py:41]: Epoch 034, Learning Rate 0.01\n","2022-03-31 10:01:47,746: INFO: [train.py:79]: Batch Loss: 0.068555 Time: 53 s\n","2022-03-31 10:01:50,209: INFO: [train.py:117]: Loss: 1.211344 Time: 2\n","2022-03-31 10:01:50,226: INFO: [train.py:41]: Epoch 035, Learning Rate 0.01\n","2022-03-31 10:02:43,357: INFO: [train.py:79]: Batch Loss: 0.080931 Time: 53 s\n","2022-03-31 10:02:45,831: INFO: [train.py:117]: Loss: 0.552164 Time: 2\n","2022-03-31 10:02:45,832: INFO: [train.py:41]: Epoch 036, Learning Rate 0.01\n","2022-03-31 10:03:39,056: INFO: [train.py:79]: Batch Loss: 0.067234 Time: 53 s\n","2022-03-31 10:03:41,542: INFO: [train.py:117]: Loss: 0.446903 Time: 2\n","2022-03-31 10:03:41,544: INFO: [train.py:41]: Epoch 037, Learning Rate 0.01\n","2022-03-31 10:04:34,710: INFO: [train.py:79]: Batch Loss: 0.058659 Time: 53 s\n","2022-03-31 10:04:37,207: INFO: [train.py:117]: Loss: 0.467572 Time: 2\n","2022-03-31 10:04:37,210: INFO: [train.py:41]: Epoch 038, Learning Rate 0.01\n","2022-03-31 10:05:30,363: INFO: [train.py:79]: Batch Loss: 0.060559 Time: 53 s\n","2022-03-31 10:05:32,844: INFO: [train.py:117]: Loss: 0.390519 Time: 2\n","2022-03-31 10:05:32,850: INFO: [train.py:41]: Epoch 039, Learning Rate 0.01\n","2022-03-31 10:06:26,114: INFO: [train.py:79]: Batch Loss: 0.055299 Time: 53 s\n","2022-03-31 10:06:28,569: INFO: [train.py:117]: Loss: 0.488514 Time: 2\n","2022-03-31 10:06:28,570: INFO: [train.py:41]: Epoch 040, Learning Rate 0.01\n","2022-03-31 10:07:21,685: INFO: [train.py:79]: Batch Loss: 0.054768 Time: 53 s\n","2022-03-31 10:07:24,132: INFO: [train.py:117]: Loss: 0.374396 Time: 2\n","2022-03-31 10:07:24,134: INFO: [train.py:41]: Epoch 041, Learning Rate 0.01\n","2022-03-31 10:08:17,161: INFO: [train.py:79]: Batch Loss: 0.052711 Time: 53 s\n","2022-03-31 10:08:19,648: INFO: [train.py:117]: Loss: 0.316364 Time: 2\n","2022-03-31 10:08:19,650: INFO: [train.py:41]: Epoch 042, Learning Rate 0.01\n","2022-03-31 10:09:12,788: INFO: [train.py:79]: Batch Loss: 0.048849 Time: 53 s\n","2022-03-31 10:09:15,202: INFO: [train.py:117]: Loss: 0.367298 Time: 2\n","2022-03-31 10:09:15,204: INFO: [train.py:41]: Epoch 043, Learning Rate 0.01\n","2022-03-31 10:10:08,282: INFO: [train.py:79]: Batch Loss: 0.048114 Time: 53 s\n","2022-03-31 10:10:10,714: INFO: [train.py:117]: Loss: 0.303088 Time: 2\n","2022-03-31 10:10:10,716: INFO: [train.py:41]: Epoch 044, Learning Rate 0.01\n","2022-03-31 10:11:03,874: INFO: [train.py:79]: Batch Loss: 0.047758 Time: 53 s\n","2022-03-31 10:11:06,342: INFO: [train.py:117]: Loss: 0.318779 Time: 2\n","2022-03-31 10:11:06,343: INFO: [train.py:41]: Epoch 045, Learning Rate 0.01\n","2022-03-31 10:11:59,611: INFO: [train.py:79]: Batch Loss: 0.046425 Time: 53 s\n","2022-03-31 10:12:02,055: INFO: [train.py:117]: Loss: 0.311572 Time: 2\n","2022-03-31 10:12:02,056: INFO: [train.py:41]: Epoch 046, Learning Rate 0.01\n","2022-03-31 10:12:55,293: INFO: [train.py:79]: Batch Loss: 0.045983 Time: 53 s\n","2022-03-31 10:12:57,729: INFO: [train.py:117]: Loss: 0.355117 Time: 2\n","2022-03-31 10:12:57,732: INFO: [train.py:41]: Epoch 047, Learning Rate 0.01\n","2022-03-31 10:13:51,030: INFO: [train.py:79]: Batch Loss: 0.044370 Time: 53 s\n","2022-03-31 10:13:53,522: INFO: [train.py:117]: Loss: 0.273879 Time: 2\n","2022-03-31 10:13:53,524: INFO: [train.py:41]: Epoch 048, Learning Rate 0.01\n","2022-03-31 10:14:46,745: INFO: [train.py:79]: Batch Loss: 0.043350 Time: 53 s\n","2022-03-31 10:14:49,196: INFO: [train.py:117]: Loss: 0.359174 Time: 2\n","2022-03-31 10:14:49,198: INFO: [train.py:41]: Epoch 049, Learning Rate 0.01\n","2022-03-31 10:15:42,412: INFO: [train.py:79]: Batch Loss: 0.047142 Time: 53 s\n","2022-03-31 10:15:44,866: INFO: [train.py:117]: Loss: 0.312739 Time: 2\n","2022-03-31 10:15:44,869: INFO: [train.py:41]: Epoch 050, Learning Rate 0.01\n","2022-03-31 10:16:38,341: INFO: [train.py:79]: Batch Loss: 0.044723 Time: 53 s\n","2022-03-31 10:16:40,809: INFO: [train.py:117]: Loss: 0.347588 Time: 2\n","2022-03-31 10:16:40,811: INFO: [train.py:41]: Epoch 051, Learning Rate 0.01\n","2022-03-31 10:17:34,400: INFO: [train.py:79]: Batch Loss: 0.043202 Time: 53 s\n","2022-03-31 10:17:36,873: INFO: [train.py:117]: Loss: 0.484347 Time: 2\n","2022-03-31 10:17:36,875: INFO: [train.py:41]: Epoch 052, Learning Rate 0.01\n","2022-03-31 10:18:30,274: INFO: [train.py:79]: Batch Loss: 0.040981 Time: 53 s\n","2022-03-31 10:18:32,724: INFO: [train.py:117]: Loss: 0.300092 Time: 2\n","2022-03-31 10:18:32,727: INFO: [train.py:41]: Epoch 053, Learning Rate 0.01\n","2022-03-31 10:19:26,043: INFO: [train.py:79]: Batch Loss: 0.042905 Time: 53 s\n","2022-03-31 10:19:28,483: INFO: [train.py:117]: Loss: 0.283782 Time: 2\n","2022-03-31 10:19:28,484: INFO: [train.py:41]: Epoch 054, Learning Rate 0.01\n","2022-03-31 10:20:21,701: INFO: [train.py:79]: Batch Loss: 0.043151 Time: 53 s\n","2022-03-31 10:20:24,207: INFO: [train.py:117]: Loss: 0.267198 Time: 2\n","2022-03-31 10:20:24,209: INFO: [train.py:41]: Epoch 055, Learning Rate 0.01\n","2022-03-31 10:21:17,679: INFO: [train.py:79]: Batch Loss: 0.041799 Time: 53 s\n","2022-03-31 10:21:20,142: INFO: [train.py:117]: Loss: 0.360099 Time: 2\n","2022-03-31 10:21:20,146: INFO: [train.py:41]: Epoch 056, Learning Rate 0.01\n","2022-03-31 10:22:13,634: INFO: [train.py:79]: Batch Loss: 0.042148 Time: 53 s\n","2022-03-31 10:22:16,162: INFO: [train.py:117]: Loss: 0.271482 Time: 2\n","2022-03-31 10:22:16,165: INFO: [train.py:41]: Epoch 057, Learning Rate 0.01\n","2022-03-31 10:23:09,359: INFO: [train.py:79]: Batch Loss: 0.041751 Time: 53 s\n","2022-03-31 10:23:11,808: INFO: [train.py:117]: Loss: 0.285163 Time: 2\n","2022-03-31 10:23:11,810: INFO: [train.py:41]: Epoch 058, Learning Rate 0.01\n","2022-03-31 10:24:04,954: INFO: [train.py:79]: Batch Loss: 0.041442 Time: 53 s\n","2022-03-31 10:24:07,410: INFO: [train.py:117]: Loss: 0.254264 Time: 2\n","2022-03-31 10:24:07,412: INFO: [train.py:41]: Epoch 059, Learning Rate 0.01\n","2022-03-31 10:25:00,653: INFO: [train.py:79]: Batch Loss: 0.042047 Time: 53 s\n","2022-03-31 10:25:03,120: INFO: [train.py:117]: Loss: 0.258078 Time: 2\n","2022-03-31 10:25:03,122: INFO: [train.py:41]: Epoch 060, Learning Rate 0.01\n","2022-03-31 10:25:56,403: INFO: [train.py:79]: Batch Loss: 0.045009 Time: 53 s\n","2022-03-31 10:25:58,885: INFO: [train.py:117]: Loss: 0.476553 Time: 2\n","2022-03-31 10:25:58,887: INFO: [train.py:41]: Epoch 061, Learning Rate 0.01\n","2022-03-31 10:26:52,075: INFO: [train.py:79]: Batch Loss: 0.041821 Time: 53 s\n","2022-03-31 10:26:54,551: INFO: [train.py:117]: Loss: 0.244968 Time: 2\n","2022-03-31 10:26:54,555: INFO: [train.py:41]: Epoch 062, Learning Rate 0.01\n","2022-03-31 10:27:47,642: INFO: [train.py:79]: Batch Loss: 0.040061 Time: 53 s\n","2022-03-31 10:27:50,139: INFO: [train.py:117]: Loss: 0.245813 Time: 2\n","2022-03-31 10:27:50,143: INFO: [train.py:41]: Epoch 063, Learning Rate 0.01\n","2022-03-31 10:28:43,264: INFO: [train.py:79]: Batch Loss: 0.039799 Time: 53 s\n","2022-03-31 10:28:45,732: INFO: [train.py:117]: Loss: 0.237887 Time: 2\n","2022-03-31 10:28:45,735: INFO: [train.py:41]: Epoch 064, Learning Rate 0.01\n","2022-03-31 10:29:39,163: INFO: [train.py:79]: Batch Loss: 0.038883 Time: 53 s\n","2022-03-31 10:29:41,664: INFO: [train.py:117]: Loss: 0.342477 Time: 2\n","2022-03-31 10:29:41,666: INFO: [train.py:41]: Epoch 065, Learning Rate 0.01\n","2022-03-31 10:30:35,136: INFO: [train.py:79]: Batch Loss: 0.040393 Time: 53 s\n","2022-03-31 10:30:37,646: INFO: [train.py:117]: Loss: 0.269986 Time: 2\n","2022-03-31 10:30:37,648: INFO: [train.py:41]: Epoch 066, Learning Rate 0.01\n","2022-03-31 10:31:31,024: INFO: [train.py:79]: Batch Loss: 0.038499 Time: 53 s\n","2022-03-31 10:31:33,509: INFO: [train.py:117]: Loss: 0.286019 Time: 2\n","2022-03-31 10:31:33,522: INFO: [train.py:41]: Epoch 067, Learning Rate 0.01\n","2022-03-31 10:32:27,009: INFO: [train.py:79]: Batch Loss: 0.038175 Time: 53 s\n","2022-03-31 10:32:29,490: INFO: [train.py:117]: Loss: 0.253996 Time: 2\n","2022-03-31 10:32:29,495: INFO: [train.py:41]: Epoch 068, Learning Rate 0.01\n","2022-03-31 10:33:22,750: INFO: [train.py:79]: Batch Loss: 0.037063 Time: 53 s\n","2022-03-31 10:33:25,211: INFO: [train.py:117]: Loss: 0.219661 Time: 2\n","2022-03-31 10:33:25,213: INFO: [train.py:41]: Epoch 069, Learning Rate 0.01\n","2022-03-31 10:34:18,493: INFO: [train.py:79]: Batch Loss: 0.036431 Time: 53 s\n","2022-03-31 10:34:20,982: INFO: [train.py:117]: Loss: 0.305467 Time: 2\n","2022-03-31 10:34:20,985: INFO: [train.py:41]: Epoch 070, Learning Rate 0.01\n","2022-03-31 10:35:14,230: INFO: [train.py:79]: Batch Loss: 0.036282 Time: 53 s\n","2022-03-31 10:35:16,804: INFO: [train.py:117]: Loss: 0.276720 Time: 2\n","2022-03-31 10:35:16,806: INFO: [train.py:41]: Epoch 071, Learning Rate 0.01\n","2022-03-31 10:36:10,160: INFO: [train.py:79]: Batch Loss: 0.036408 Time: 53 s\n","2022-03-31 10:36:12,675: INFO: [train.py:117]: Loss: 0.245087 Time: 2\n","2022-03-31 10:36:12,678: INFO: [train.py:41]: Epoch 072, Learning Rate 0.01\n","2022-03-31 10:37:06,052: INFO: [train.py:79]: Batch Loss: 0.036004 Time: 53 s\n","2022-03-31 10:37:08,547: INFO: [train.py:117]: Loss: 0.274008 Time: 2\n","2022-03-31 10:37:08,549: INFO: [train.py:41]: Epoch 073, Learning Rate 0.01\n","2022-03-31 10:38:01,842: INFO: [train.py:79]: Batch Loss: 0.036134 Time: 53 s\n","2022-03-31 10:38:04,309: INFO: [train.py:117]: Loss: 0.225462 Time: 2\n","2022-03-31 10:38:04,310: INFO: [train.py:41]: Epoch 074, Learning Rate 0.01\n","2022-03-31 10:38:57,480: INFO: [train.py:79]: Batch Loss: 0.035637 Time: 53 s\n","2022-03-31 10:38:59,986: INFO: [train.py:117]: Loss: 0.238119 Time: 2\n","2022-03-31 10:38:59,988: INFO: [train.py:41]: Epoch 075, Learning Rate 0.01\n","2022-03-31 10:39:53,326: INFO: [train.py:79]: Batch Loss: 0.035052 Time: 53 s\n","2022-03-31 10:39:55,884: INFO: [train.py:117]: Loss: 0.236958 Time: 2\n","2022-03-31 10:39:55,886: INFO: [train.py:41]: Epoch 076, Learning Rate 0.01\n","2022-03-31 10:40:49,497: INFO: [train.py:79]: Batch Loss: 0.034825 Time: 53 s\n","2022-03-31 10:40:51,996: INFO: [train.py:117]: Loss: 0.361160 Time: 2\n","2022-03-31 10:40:51,999: INFO: [train.py:41]: Epoch 077, Learning Rate 0.01\n","2022-03-31 10:41:45,417: INFO: [train.py:79]: Batch Loss: 0.034450 Time: 53 s\n","2022-03-31 10:41:47,949: INFO: [train.py:117]: Loss: 0.269558 Time: 2\n","2022-03-31 10:41:47,951: INFO: [train.py:41]: Epoch 078, Learning Rate 0.01\n","2022-03-31 10:42:41,417: INFO: [train.py:79]: Batch Loss: 0.034618 Time: 53 s\n","2022-03-31 10:42:43,943: INFO: [train.py:117]: Loss: 0.248094 Time: 2\n","2022-03-31 10:42:43,944: INFO: [train.py:41]: Epoch 079, Learning Rate 0.01\n","2022-03-31 10:43:37,336: INFO: [train.py:79]: Batch Loss: 0.034574 Time: 53 s\n","2022-03-31 10:43:39,884: INFO: [train.py:117]: Loss: 0.277049 Time: 2\n","2022-03-31 10:43:39,886: INFO: [train.py:41]: Epoch 080, Learning Rate 0.01\n","2022-03-31 10:44:33,325: INFO: [train.py:79]: Batch Loss: 0.034059 Time: 53 s\n","2022-03-31 10:44:35,882: INFO: [train.py:117]: Loss: 0.251859 Time: 2\n"]}]},{"cell_type":"code","source":["import re\n","train_log ='''2022-03-31 09:26:47,027: INFO: [train.py:41]: Epoch 001, Learning Rate 0.01\n","2022-03-31 09:30:39,645: INFO: [train.py:79]: Batch Loss: 0.336162 Time: 232 s\n","2022-03-31 09:31:00,706: INFO: [train.py:117]: Loss: 1.958754 Time: 21\n","2022-03-31 09:31:00,708: INFO: [train.py:41]: Epoch 002, Learning Rate 0.01\n","2022-03-31 09:31:54,645: INFO: [train.py:79]: Batch Loss: 0.090083 Time: 53 s\n","2022-03-31 09:31:57,140: INFO: [train.py:117]: Loss: 0.614533 Time: 2\n","2022-03-31 09:31:57,142: INFO: [train.py:41]: Epoch 003, Learning Rate 0.01\n","2022-03-31 09:32:50,722: INFO: [train.py:79]: Batch Loss: 0.075674 Time: 53 s\n","2022-03-31 09:32:53,217: INFO: [train.py:117]: Loss: 0.419660 Time: 2\n","2022-03-31 09:32:53,220: INFO: [train.py:41]: Epoch 004, Learning Rate 0.01\n","2022-03-31 09:33:46,730: INFO: [train.py:79]: Batch Loss: 0.062530 Time: 53 s\n","2022-03-31 09:33:49,248: INFO: [train.py:117]: Loss: 0.339584 Time: 2\n","2022-03-31 09:33:49,250: INFO: [train.py:41]: Epoch 005, Learning Rate 0.01\n","2022-03-31 09:34:42,713: INFO: [train.py:79]: Batch Loss: 0.056951 Time: 53 s\n","2022-03-31 09:34:45,210: INFO: [train.py:117]: Loss: 0.417431 Time: 2\n","2022-03-31 09:34:45,212: INFO: [train.py:41]: Epoch 006, Learning Rate 0.01\n","2022-03-31 09:35:39,045: INFO: [train.py:79]: Batch Loss: 0.053436 Time: 53 s\n","2022-03-31 09:35:41,549: INFO: [train.py:117]: Loss: 0.275865 Time: 2\n","2022-03-31 09:35:41,552: INFO: [train.py:41]: Epoch 007, Learning Rate 0.01\n","2022-03-31 09:36:35,135: INFO: [train.py:79]: Batch Loss: 0.049958 Time: 53 s\n","2022-03-31 09:36:37,674: INFO: [train.py:117]: Loss: 0.549373 Time: 2\n","2022-03-31 09:36:37,676: INFO: [train.py:41]: Epoch 008, Learning Rate 0.01\n","2022-03-31 09:37:31,233: INFO: [train.py:79]: Batch Loss: 0.050326 Time: 53 s\n","2022-03-31 09:37:33,690: INFO: [train.py:117]: Loss: 0.299007 Time: 2\n","2022-03-31 09:37:33,692: INFO: [train.py:41]: Epoch 009, Learning Rate 0.01\n","2022-03-31 09:38:27,078: INFO: [train.py:79]: Batch Loss: 0.048154 Time: 53 s\n","2022-03-31 09:38:29,545: INFO: [train.py:117]: Loss: 0.272130 Time: 2\n","2022-03-31 09:38:29,547: INFO: [train.py:41]: Epoch 010, Learning Rate 0.01\n","2022-03-31 09:39:23,020: INFO: [train.py:79]: Batch Loss: 0.044942 Time: 53 s\n","2022-03-31 09:39:25,520: INFO: [train.py:117]: Loss: 0.380452 Time: 2\n","2022-03-31 09:39:25,523: INFO: [train.py:41]: Epoch 011, Learning Rate 0.01\n","2022-03-31 09:40:18,952: INFO: [train.py:79]: Batch Loss: 0.042991 Time: 53 s\n","2022-03-31 09:40:21,399: INFO: [train.py:117]: Loss: 0.268137 Time: 2\n","2022-03-31 09:40:21,402: INFO: [train.py:41]: Epoch 012, Learning Rate 0.01\n","2022-03-31 09:41:14,910: INFO: [train.py:79]: Batch Loss: 0.043378 Time: 53 s\n","2022-03-31 09:41:17,402: INFO: [train.py:117]: Loss: 33656.521565 Time: 2\n","2022-03-31 09:41:17,404: INFO: [train.py:41]: Epoch 013, Learning Rate 0.01\n","2022-03-31 09:42:10,935: INFO: [train.py:79]: Batch Loss: 0.044172 Time: 53 s\n","2022-03-31 09:42:13,429: INFO: [train.py:117]: Loss: 14.354378 Time: 2\n","2022-03-31 09:42:13,431: INFO: [train.py:41]: Epoch 014, Learning Rate 0.01\n","2022-03-31 09:43:06,824: INFO: [train.py:79]: Batch Loss: 0.044962 Time: 53 s\n","2022-03-31 09:43:09,302: INFO: [train.py:117]: Loss: 0.331448 Time: 2\n","2022-03-31 09:43:09,303: INFO: [train.py:41]: Epoch 015, Learning Rate 0.01\n","2022-03-31 09:44:02,857: INFO: [train.py:79]: Batch Loss: 0.040770 Time: 53 s\n","2022-03-31 09:44:05,360: INFO: [train.py:117]: Loss: 0.336705 Time: 2\n","2022-03-31 09:44:05,367: INFO: [train.py:41]: Epoch 016, Learning Rate 0.01\n","2022-03-31 09:44:58,797: INFO: [train.py:79]: Batch Loss: 0.039026 Time: 53 s\n","2022-03-31 09:45:01,263: INFO: [train.py:117]: Loss: 3.157186 Time: 2\n","2022-03-31 09:45:01,266: INFO: [train.py:41]: Epoch 017, Learning Rate 0.01\n","2022-03-31 09:45:54,719: INFO: [train.py:79]: Batch Loss: 0.040219 Time: 53 s\n","2022-03-31 09:45:57,201: INFO: [train.py:117]: Loss: 0.559086 Time: 2\n","2022-03-31 09:45:57,204: INFO: [train.py:41]: Epoch 018, Learning Rate 0.01\n","2022-03-31 09:46:50,875: INFO: [train.py:79]: Batch Loss: 0.038775 Time: 53 s\n","2022-03-31 09:46:53,358: INFO: [train.py:117]: Loss: 780859072.225799 Time: 2\n","2022-03-31 09:46:53,365: INFO: [train.py:41]: Epoch 019, Learning Rate 0.01\n","2022-03-31 09:47:46,967: INFO: [train.py:79]: Batch Loss: 0.040282 Time: 53 s\n","2022-03-31 09:47:49,498: INFO: [train.py:117]: Loss: 0.433079 Time: 2\n","2022-03-31 09:47:49,501: INFO: [train.py:41]: Epoch 020, Learning Rate 0.01\n","2022-03-31 09:48:43,168: INFO: [train.py:79]: Batch Loss: 0.037008 Time: 53 s\n","2022-03-31 09:48:45,704: INFO: [train.py:117]: Loss: 0.313703 Time: 2\n","2022-03-31 09:48:45,706: INFO: [train.py:41]: Epoch 021, Learning Rate 0.01\n","2022-03-31 09:49:39,230: INFO: [train.py:79]: Batch Loss: 0.037069 Time: 53 s\n","2022-03-31 09:49:41,711: INFO: [train.py:117]: Loss: 0.370599 Time: 2\n","2022-03-31 09:49:41,714: INFO: [train.py:41]: Epoch 022, Learning Rate 0.01\n","2022-03-31 09:50:35,318: INFO: [train.py:79]: Batch Loss: 0.036788 Time: 53 s\n","2022-03-31 09:50:37,832: INFO: [train.py:117]: Loss: 0.306370 Time: 2\n","2022-03-31 09:50:37,834: INFO: [train.py:41]: Epoch 023, Learning Rate 0.01\n","2022-03-31 09:51:31,463: INFO: [train.py:79]: Batch Loss: 0.035557 Time: 53 s\n","2022-03-31 09:51:34,001: INFO: [train.py:117]: Loss: 0.510162 Time: 2\n","2022-03-31 09:51:34,008: INFO: [train.py:41]: Epoch 024, Learning Rate 0.01\n","2022-03-31 09:52:27,631: INFO: [train.py:79]: Batch Loss: 0.035513 Time: 53 s\n","2022-03-31 09:52:30,167: INFO: [train.py:117]: Loss: 0.307638 Time: 2\n","2022-03-31 09:52:30,170: INFO: [train.py:41]: Epoch 025, Learning Rate 0.01\n","2022-03-31 09:53:24,035: INFO: [train.py:79]: Batch Loss: 0.032559 Time: 53 s\n","2022-03-31 09:53:26,538: INFO: [train.py:117]: Loss: 0.446683 Time: 2\n","2022-03-31 09:53:26,542: INFO: [train.py:41]: Epoch 026, Learning Rate 0.01\n","2022-03-31 09:54:20,145: INFO: [train.py:79]: Batch Loss: 0.031903 Time: 53 s\n","2022-03-31 09:54:22,644: INFO: [train.py:117]: Loss: 0.450164 Time: 2\n","2022-03-31 09:54:22,646: INFO: [train.py:41]: Epoch 027, Learning Rate 0.01\n","2022-03-31 09:55:16,048: INFO: [train.py:79]: Batch Loss: 0.030459 Time: 53 s\n","2022-03-31 09:55:18,567: INFO: [train.py:117]: Loss: 0.339369 Time: 2\n","2022-03-31 09:55:18,569: INFO: [train.py:41]: Epoch 028, Learning Rate 0.01\n","2022-03-31 09:56:12,271: INFO: [train.py:79]: Batch Loss: 0.030944 Time: 53 s\n","2022-03-31 09:56:14,774: INFO: [train.py:117]: Loss: 0.404924 Time: 2\n","2022-03-31 09:56:14,776: INFO: [train.py:41]: Epoch 029, Learning Rate 0.01\n","2022-03-31 09:57:08,157: INFO: [train.py:79]: Batch Loss: 0.034727 Time: 53 s\n","2022-03-31 09:57:10,635: INFO: [train.py:117]: Loss: 0.268221 Time: 2\n","2022-03-31 09:57:10,641: INFO: [train.py:41]: Epoch 030, Learning Rate 0.01\n","2022-03-31 09:58:04,185: INFO: [train.py:79]: Batch Loss: 0.034260 Time: 53 s\n","2022-03-31 09:58:06,686: INFO: [train.py:117]: Loss: 0.395160 Time: 2\n","2022-03-31 09:58:06,688: INFO: [train.py:41]: Epoch 031, Learning Rate 0.01\n","2022-03-31 09:59:00,129: INFO: [train.py:79]: Batch Loss: 0.039822 Time: 53 s\n","2022-03-31 09:59:02,622: INFO: [train.py:117]: Loss: 0.513978 Time: 2\n","2022-03-31 09:59:02,626: INFO: [train.py:41]: Epoch 032, Learning Rate 0.01\n","2022-03-31 09:59:56,101: INFO: [train.py:79]: Batch Loss: 0.039713 Time: 53 s\n","2022-03-31 09:59:58,554: INFO: [train.py:117]: Loss: 0.445592 Time: 2\n","2022-03-31 09:59:58,555: INFO: [train.py:41]: Epoch 033, Learning Rate 0.01\n","2022-03-31 10:00:51,935: INFO: [train.py:79]: Batch Loss: 0.037452 Time: 53 s\n","2022-03-31 10:00:54,390: INFO: [train.py:117]: Loss: 0.887513 Time: 2\n","2022-03-31 10:00:54,393: INFO: [train.py:41]: Epoch 034, Learning Rate 0.01\n","2022-03-31 10:01:47,746: INFO: [train.py:79]: Batch Loss: 0.068555 Time: 53 s\n","2022-03-31 10:01:50,209: INFO: [train.py:117]: Loss: 1.211344 Time: 2\n","2022-03-31 10:01:50,226: INFO: [train.py:41]: Epoch 035, Learning Rate 0.01\n","2022-03-31 10:02:43,357: INFO: [train.py:79]: Batch Loss: 0.080931 Time: 53 s\n","2022-03-31 10:02:45,831: INFO: [train.py:117]: Loss: 0.552164 Time: 2\n","2022-03-31 10:02:45,832: INFO: [train.py:41]: Epoch 036, Learning Rate 0.01\n","2022-03-31 10:03:39,056: INFO: [train.py:79]: Batch Loss: 0.067234 Time: 53 s\n","2022-03-31 10:03:41,542: INFO: [train.py:117]: Loss: 0.446903 Time: 2\n","2022-03-31 10:03:41,544: INFO: [train.py:41]: Epoch 037, Learning Rate 0.01\n","2022-03-31 10:04:34,710: INFO: [train.py:79]: Batch Loss: 0.058659 Time: 53 s\n","2022-03-31 10:04:37,207: INFO: [train.py:117]: Loss: 0.467572 Time: 2\n","2022-03-31 10:04:37,210: INFO: [train.py:41]: Epoch 038, Learning Rate 0.01\n","2022-03-31 10:05:30,363: INFO: [train.py:79]: Batch Loss: 0.060559 Time: 53 s\n","2022-03-31 10:05:32,844: INFO: [train.py:117]: Loss: 0.390519 Time: 2\n","2022-03-31 10:05:32,850: INFO: [train.py:41]: Epoch 039, Learning Rate 0.01\n","2022-03-31 10:06:26,114: INFO: [train.py:79]: Batch Loss: 0.055299 Time: 53 s\n","2022-03-31 10:06:28,569: INFO: [train.py:117]: Loss: 0.488514 Time: 2\n","2022-03-31 10:06:28,570: INFO: [train.py:41]: Epoch 040, Learning Rate 0.01\n","2022-03-31 10:07:21,685: INFO: [train.py:79]: Batch Loss: 0.054768 Time: 53 s\n","2022-03-31 10:07:24,132: INFO: [train.py:117]: Loss: 0.374396 Time: 2\n","2022-03-31 10:07:24,134: INFO: [train.py:41]: Epoch 041, Learning Rate 0.01\n","2022-03-31 10:08:17,161: INFO: [train.py:79]: Batch Loss: 0.052711 Time: 53 s\n","2022-03-31 10:08:19,648: INFO: [train.py:117]: Loss: 0.316364 Time: 2\n","2022-03-31 10:08:19,650: INFO: [train.py:41]: Epoch 042, Learning Rate 0.01\n","2022-03-31 10:09:12,788: INFO: [train.py:79]: Batch Loss: 0.048849 Time: 53 s\n","2022-03-31 10:09:15,202: INFO: [train.py:117]: Loss: 0.367298 Time: 2\n","2022-03-31 10:09:15,204: INFO: [train.py:41]: Epoch 043, Learning Rate 0.01\n","2022-03-31 10:10:08,282: INFO: [train.py:79]: Batch Loss: 0.048114 Time: 53 s\n","2022-03-31 10:10:10,714: INFO: [train.py:117]: Loss: 0.303088 Time: 2\n","2022-03-31 10:10:10,716: INFO: [train.py:41]: Epoch 044, Learning Rate 0.01\n","2022-03-31 10:11:03,874: INFO: [train.py:79]: Batch Loss: 0.047758 Time: 53 s\n","2022-03-31 10:11:06,342: INFO: [train.py:117]: Loss: 0.318779 Time: 2\n","2022-03-31 10:11:06,343: INFO: [train.py:41]: Epoch 045, Learning Rate 0.01\n","2022-03-31 10:11:59,611: INFO: [train.py:79]: Batch Loss: 0.046425 Time: 53 s\n","2022-03-31 10:12:02,055: INFO: [train.py:117]: Loss: 0.311572 Time: 2\n","2022-03-31 10:12:02,056: INFO: [train.py:41]: Epoch 046, Learning Rate 0.01\n","2022-03-31 10:12:55,293: INFO: [train.py:79]: Batch Loss: 0.045983 Time: 53 s\n","2022-03-31 10:12:57,729: INFO: [train.py:117]: Loss: 0.355117 Time: 2\n","2022-03-31 10:12:57,732: INFO: [train.py:41]: Epoch 047, Learning Rate 0.01\n","2022-03-31 10:13:51,030: INFO: [train.py:79]: Batch Loss: 0.044370 Time: 53 s\n","2022-03-31 10:13:53,522: INFO: [train.py:117]: Loss: 0.273879 Time: 2\n","2022-03-31 10:13:53,524: INFO: [train.py:41]: Epoch 048, Learning Rate 0.01\n","2022-03-31 10:14:46,745: INFO: [train.py:79]: Batch Loss: 0.043350 Time: 53 s\n","2022-03-31 10:14:49,196: INFO: [train.py:117]: Loss: 0.359174 Time: 2\n","2022-03-31 10:14:49,198: INFO: [train.py:41]: Epoch 049, Learning Rate 0.01\n","2022-03-31 10:15:42,412: INFO: [train.py:79]: Batch Loss: 0.047142 Time: 53 s\n","2022-03-31 10:15:44,866: INFO: [train.py:117]: Loss: 0.312739 Time: 2\n","2022-03-31 10:15:44,869: INFO: [train.py:41]: Epoch 050, Learning Rate 0.01\n","2022-03-31 10:16:38,341: INFO: [train.py:79]: Batch Loss: 0.044723 Time: 53 s\n","2022-03-31 10:16:40,809: INFO: [train.py:117]: Loss: 0.347588 Time: 2\n","2022-03-31 10:16:40,811: INFO: [train.py:41]: Epoch 051, Learning Rate 0.01\n","2022-03-31 10:17:34,400: INFO: [train.py:79]: Batch Loss: 0.043202 Time: 53 s\n","2022-03-31 10:17:36,873: INFO: [train.py:117]: Loss: 0.484347 Time: 2\n","2022-03-31 10:17:36,875: INFO: [train.py:41]: Epoch 052, Learning Rate 0.01\n","2022-03-31 10:18:30,274: INFO: [train.py:79]: Batch Loss: 0.040981 Time: 53 s\n","2022-03-31 10:18:32,724: INFO: [train.py:117]: Loss: 0.300092 Time: 2\n","2022-03-31 10:18:32,727: INFO: [train.py:41]: Epoch 053, Learning Rate 0.01\n","2022-03-31 10:19:26,043: INFO: [train.py:79]: Batch Loss: 0.042905 Time: 53 s\n","2022-03-31 10:19:28,483: INFO: [train.py:117]: Loss: 0.283782 Time: 2\n","2022-03-31 10:19:28,484: INFO: [train.py:41]: Epoch 054, Learning Rate 0.01\n","2022-03-31 10:20:21,701: INFO: [train.py:79]: Batch Loss: 0.043151 Time: 53 s\n","2022-03-31 10:20:24,207: INFO: [train.py:117]: Loss: 0.267198 Time: 2\n","2022-03-31 10:20:24,209: INFO: [train.py:41]: Epoch 055, Learning Rate 0.01\n","2022-03-31 10:21:17,679: INFO: [train.py:79]: Batch Loss: 0.041799 Time: 53 s\n","2022-03-31 10:21:20,142: INFO: [train.py:117]: Loss: 0.360099 Time: 2\n","2022-03-31 10:21:20,146: INFO: [train.py:41]: Epoch 056, Learning Rate 0.01\n","2022-03-31 10:22:13,634: INFO: [train.py:79]: Batch Loss: 0.042148 Time: 53 s\n","2022-03-31 10:22:16,162: INFO: [train.py:117]: Loss: 0.271482 Time: 2\n","2022-03-31 10:22:16,165: INFO: [train.py:41]: Epoch 057, Learning Rate 0.01\n","2022-03-31 10:23:09,359: INFO: [train.py:79]: Batch Loss: 0.041751 Time: 53 s\n","2022-03-31 10:23:11,808: INFO: [train.py:117]: Loss: 0.285163 Time: 2\n","2022-03-31 10:23:11,810: INFO: [train.py:41]: Epoch 058, Learning Rate 0.01\n","2022-03-31 10:24:04,954: INFO: [train.py:79]: Batch Loss: 0.041442 Time: 53 s\n","2022-03-31 10:24:07,410: INFO: [train.py:117]: Loss: 0.254264 Time: 2\n","2022-03-31 10:24:07,412: INFO: [train.py:41]: Epoch 059, Learning Rate 0.01\n","2022-03-31 10:25:00,653: INFO: [train.py:79]: Batch Loss: 0.042047 Time: 53 s\n","2022-03-31 10:25:03,120: INFO: [train.py:117]: Loss: 0.258078 Time: 2\n","2022-03-31 10:25:03,122: INFO: [train.py:41]: Epoch 060, Learning Rate 0.01\n","2022-03-31 10:25:56,403: INFO: [train.py:79]: Batch Loss: 0.045009 Time: 53 s\n","2022-03-31 10:25:58,885: INFO: [train.py:117]: Loss: 0.476553 Time: 2\n","2022-03-31 10:25:58,887: INFO: [train.py:41]: Epoch 061, Learning Rate 0.01\n","2022-03-31 10:26:52,075: INFO: [train.py:79]: Batch Loss: 0.041821 Time: 53 s\n","2022-03-31 10:26:54,551: INFO: [train.py:117]: Loss: 0.244968 Time: 2\n","2022-03-31 10:26:54,555: INFO: [train.py:41]: Epoch 062, Learning Rate 0.01\n","2022-03-31 10:27:47,642: INFO: [train.py:79]: Batch Loss: 0.040061 Time: 53 s\n","2022-03-31 10:27:50,139: INFO: [train.py:117]: Loss: 0.245813 Time: 2\n","2022-03-31 10:27:50,143: INFO: [train.py:41]: Epoch 063, Learning Rate 0.01\n","2022-03-31 10:28:43,264: INFO: [train.py:79]: Batch Loss: 0.039799 Time: 53 s\n","2022-03-31 10:28:45,732: INFO: [train.py:117]: Loss: 0.237887 Time: 2\n","2022-03-31 10:28:45,735: INFO: [train.py:41]: Epoch 064, Learning Rate 0.01\n","2022-03-31 10:29:39,163: INFO: [train.py:79]: Batch Loss: 0.038883 Time: 53 s\n","2022-03-31 10:29:41,664: INFO: [train.py:117]: Loss: 0.342477 Time: 2\n","2022-03-31 10:29:41,666: INFO: [train.py:41]: Epoch 065, Learning Rate 0.01\n","2022-03-31 10:30:35,136: INFO: [train.py:79]: Batch Loss: 0.040393 Time: 53 s\n","2022-03-31 10:30:37,646: INFO: [train.py:117]: Loss: 0.269986 Time: 2\n","2022-03-31 10:30:37,648: INFO: [train.py:41]: Epoch 066, Learning Rate 0.01\n","2022-03-31 10:31:31,024: INFO: [train.py:79]: Batch Loss: 0.038499 Time: 53 s\n","2022-03-31 10:31:33,509: INFO: [train.py:117]: Loss: 0.286019 Time: 2\n","2022-03-31 10:31:33,522: INFO: [train.py:41]: Epoch 067, Learning Rate 0.01\n","2022-03-31 10:32:27,009: INFO: [train.py:79]: Batch Loss: 0.038175 Time: 53 s\n","2022-03-31 10:32:29,490: INFO: [train.py:117]: Loss: 0.253996 Time: 2\n","2022-03-31 10:32:29,495: INFO: [train.py:41]: Epoch 068, Learning Rate 0.01\n","2022-03-31 10:33:22,750: INFO: [train.py:79]: Batch Loss: 0.037063 Time: 53 s\n","2022-03-31 10:33:25,211: INFO: [train.py:117]: Loss: 0.219661 Time: 2\n","2022-03-31 10:33:25,213: INFO: [train.py:41]: Epoch 069, Learning Rate 0.01\n","2022-03-31 10:34:18,493: INFO: [train.py:79]: Batch Loss: 0.036431 Time: 53 s\n","2022-03-31 10:34:20,982: INFO: [train.py:117]: Loss: 0.305467 Time: 2\n","2022-03-31 10:34:20,985: INFO: [train.py:41]: Epoch 070, Learning Rate 0.01\n","2022-03-31 10:35:14,230: INFO: [train.py:79]: Batch Loss: 0.036282 Time: 53 s\n","2022-03-31 10:35:16,804: INFO: [train.py:117]: Loss: 0.276720 Time: 2\n","2022-03-31 10:35:16,806: INFO: [train.py:41]: Epoch 071, Learning Rate 0.01\n","2022-03-31 10:36:10,160: INFO: [train.py:79]: Batch Loss: 0.036408 Time: 53 s\n","2022-03-31 10:36:12,675: INFO: [train.py:117]: Loss: 0.245087 Time: 2\n","2022-03-31 10:36:12,678: INFO: [train.py:41]: Epoch 072, Learning Rate 0.01\n","2022-03-31 10:37:06,052: INFO: [train.py:79]: Batch Loss: 0.036004 Time: 53 s\n","2022-03-31 10:37:08,547: INFO: [train.py:117]: Loss: 0.274008 Time: 2\n","2022-03-31 10:37:08,549: INFO: [train.py:41]: Epoch 073, Learning Rate 0.01\n","2022-03-31 10:38:01,842: INFO: [train.py:79]: Batch Loss: 0.036134 Time: 53 s\n","2022-03-31 10:38:04,309: INFO: [train.py:117]: Loss: 0.225462 Time: 2\n","2022-03-31 10:38:04,310: INFO: [train.py:41]: Epoch 074, Learning Rate 0.01\n","2022-03-31 10:38:57,480: INFO: [train.py:79]: Batch Loss: 0.035637 Time: 53 s\n","2022-03-31 10:38:59,986: INFO: [train.py:117]: Loss: 0.238119 Time: 2\n","2022-03-31 10:38:59,988: INFO: [train.py:41]: Epoch 075, Learning Rate 0.01\n","2022-03-31 10:39:53,326: INFO: [train.py:79]: Batch Loss: 0.035052 Time: 53 s\n","2022-03-31 10:39:55,884: INFO: [train.py:117]: Loss: 0.236958 Time: 2\n","2022-03-31 10:39:55,886: INFO: [train.py:41]: Epoch 076, Learning Rate 0.01\n","2022-03-31 10:40:49,497: INFO: [train.py:79]: Batch Loss: 0.034825 Time: 53 s\n","2022-03-31 10:40:51,996: INFO: [train.py:117]: Loss: 0.361160 Time: 2\n","2022-03-31 10:40:51,999: INFO: [train.py:41]: Epoch 077, Learning Rate 0.01\n","2022-03-31 10:41:45,417: INFO: [train.py:79]: Batch Loss: 0.034450 Time: 53 s\n","2022-03-31 10:41:47,949: INFO: [train.py:117]: Loss: 0.269558 Time: 2\n","2022-03-31 10:41:47,951: INFO: [train.py:41]: Epoch 078, Learning Rate 0.01\n","2022-03-31 10:42:41,417: INFO: [train.py:79]: Batch Loss: 0.034618 Time: 53 s\n","2022-03-31 10:42:43,943: INFO: [train.py:117]: Loss: 0.248094 Time: 2\n","2022-03-31 10:42:43,944: INFO: [train.py:41]: Epoch 079, Learning Rate 0.01\n","2022-03-31 10:43:37,336: INFO: [train.py:79]: Batch Loss: 0.034574 Time: 53 s\n","2022-03-31 10:43:39,884: INFO: [train.py:117]: Loss: 0.277049 Time: 2\n","2022-03-31 10:43:39,886: INFO: [train.py:41]: Epoch 080, Learning Rate 0.01\n","2022-03-31 10:44:33,325: INFO: [train.py:79]: Batch Loss: 0.034059 Time: 53 s\n","2022-03-31 10:44:35,882: INFO: [train.py:117]: Loss: 0.251859 Time: 2'''\n","re_1 = re.compile('Time: \\d{1,2}\\n')\n","epoch_list = re_1.split(train_log)\n","epoch_list[79] = epoch_list[79][:-8]\n","dic = {}\n","for epoch in epoch_list:\n","  epoch_num = epoch[epoch.index('Epoch') + 6:epoch.index(', Learning Rate')]\n","  epoch_loss = epoch[epoch.rindex('Loss') + 6:]\n","  dic[epoch_num] = float(epoch_loss)\n","dic_order = sorted(dic.items(), key = lambda x:x[1])\n","print(dic_order[0][0])\n","print(dic_order[0][1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AR4IvOr2GLbU","executionInfo":{"status":"ok","timestamp":1648723748092,"user_tz":-480,"elapsed":643,"user":{"displayName":"Junyong Li","userId":"01294882104453873440"}},"outputId":"efe195bc-70ff-4c4c-b507-4ffd14df96b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["068\n","0.219661\n"]}]},{"cell_type":"code","source":["%run train.py --mode test --ckpt \"models\\068.ckpt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ix-uk13EQ0yH","executionInfo":{"status":"ok","timestamp":1648724147643,"user_tz":-480,"elapsed":22620,"user":{"displayName":"Junyong Li","userId":"01294882104453873440"}},"outputId":"7a11f499-9d4d-4598-d874-f6736a7542ff"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-03-31 10:55:27,176: INFO: [train.py:193]: Model loaded from models\\068.ckpt\n","2022-03-31 10:55:47,209: INFO: [train.py:143]: Testing Time: 19 s\n"]}]}]}